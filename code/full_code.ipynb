{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments as TrainingArguments\n",
    "from transformers import Seq2SeqTrainer as Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq as DataCollator\n",
    "from datasets import Dataset\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read CSV Dataset\n",
    "\n",
    "df = pd.read_csv(r'dataset_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove Null and Duplicate Rows\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Pretrained Model and Tokenizer\n",
    "\n",
    "model_path = 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "\n",
    "df = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adding 'summarize:' Prefix and Tokenization\n",
    "\n",
    "def preprocess(text):\n",
    "    inputs = [\"summarize: \" + a for a in text['article']]\n",
    "    input_tokens = tokenizer(inputs, padding='max_length', max_length=1024, truncation=True)\n",
    "    target_tokens = tokenizer(text_target=text['highlights'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "    labels = target_tokens['input_ids']\n",
    "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in seq]\n",
    "            for seq in labels]\n",
    "\n",
    "    input_tokens['labels'] = labels\n",
    "    return input_tokens\n",
    "\n",
    "tokenized_df = df.map(preprocess, batched=True)\n",
    "\n",
    "tokenized_df = tokenized_df.remove_columns(['article', 'highlights', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split Dataset for Training and Testing\n",
    "\n",
    "split_dataset = tokenized_df.train_test_split(test_size=0.2, seed=42)\n",
    "train_df = split_dataset['train']\n",
    "test_df = split_dataset['test']\n",
    "small_test_df = test_df.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute rouge metrics (rouge1, rouge2, rougeL, rougeLsum)\n",
    "\n",
    "rouge = load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions = decoded_preds, references = decoded_labels)\n",
    "\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    # Memory Optimization\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=4,\n",
    "    # Training\n",
    "    learning_rate=4e-5,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.05,\n",
    "    # Evaluation\n",
    "    metric_for_best_model='rougeL',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    # For GPU\n",
    "    fp16=True,\n",
    "    # Other\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adjusting Model Generation Settings\n",
    "\n",
    "model.config.min_new_tokens = 30\n",
    "model.config.max_new_tokens = 128\n",
    "model.config.num_beams = 6\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=small_test_df,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Final Evaluation\n",
    "\n",
    "final_results = trainer.evaluate(test_df)\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8664866,
     "sourceId": 13632517,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
